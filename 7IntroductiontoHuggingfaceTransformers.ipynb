{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 12612641,
          "sourceType": "datasetVersion",
          "datasetId": 7967487
        }
      ],
      "dockerImageVersionId": 31089,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saliq-2/NN-A3-assignment/blob/main/7IntroductiontoHuggingfaceTransformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers[sentencepiece] datasets evaluate sacrebleu --quiet\n",
        "!pip install --upgrade -q transformers\n"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-30T13:42:32.414329Z",
          "iopub.execute_input": "2025-07-30T13:42:32.414696Z",
          "iopub.status.idle": "2025-07-30T13:42:41.294219Z",
          "shell.execute_reply.started": "2025-07-30T13:42:32.414669Z",
          "shell.execute_reply": "2025-07-30T13:42:41.292801Z"
        },
        "id": "9oS4Y8xYrpp_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    Seq2SeqTrainer,\n",
        "    Seq2SeqTrainingArguments\n",
        ")\n",
        "import evaluate\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-30T13:42:41.296938Z",
          "iopub.execute_input": "2025-07-30T13:42:41.297332Z",
          "iopub.status.idle": "2025-07-30T13:42:53.678746Z",
          "shell.execute_reply.started": "2025-07-30T13:42:41.297299Z",
          "shell.execute_reply": "2025-07-30T13:42:53.677653Z"
        },
        "id": "hb17Q0oerpqA",
        "outputId": "1555376e-5aa6-4e05-ea78-fc58c34e7782"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "2025-07-30 13:42:48.645868: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1753882968.675976     208 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1753882968.684555     208 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "# Load EN and DE text files\n",
        "with open(\"/kaggle/input/europarl-train-data/dataset.train.en\", encoding=\"ISO-8859-1\") as f:\n",
        "    en_lines = f.read().strip().split(\"\\n\")\n",
        "\n",
        "with open(\"/kaggle/input/europarl-train-data/dataset.train.de\", encoding=\"ISO-8859-1\") as f:\n",
        "    de_lines = f.read().strip().split(\"\\n\")\n",
        "\n",
        "# Ensuring equal length\n",
        "min_len = min(len(en_lines), len(de_lines))\n",
        "en_lines, de_lines = en_lines[:min_len], de_lines[:min_len]\n",
        "\n",
        "# Prepare dataset format\n",
        "data = [{\"translation\": {\"en\": en, \"de\": de}} for en, de in zip(en_lines, de_lines)]\n",
        "\n",
        "# Create Hugging Face dataset and split\n",
        "raw_dataset = Dataset.from_list(data).train_test_split(test_size=0.1)\n",
        "\n",
        "print(raw_dataset)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-30T13:42:53.679727Z",
          "iopub.execute_input": "2025-07-30T13:42:53.680452Z",
          "iopub.status.idle": "2025-07-30T13:42:53.724075Z",
          "shell.execute_reply.started": "2025-07-30T13:42:53.680423Z",
          "shell.execute_reply": "2025-07-30T13:42:53.723284Z"
        },
        "id": "Iokqb28nrpqB",
        "outputId": "dca7b97b-1b2d-4428-8cac-9cb7371cef32"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "DatasetDict({\n    train: Dataset({\n        features: ['translation'],\n        num_rows: 1800\n    })\n    test: Dataset({\n        features: ['translation'],\n        num_rows: 201\n    })\n})\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model_checkpoint = \"Helsinki-NLP/opus-mt-en-de\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "source_lang = \"en\"\n",
        "target_lang = \"de\"\n",
        "prefix = \"\"\n",
        "\n",
        "max_input_length = 128\n",
        "max_target_length = 128\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = [prefix + ex[source_lang] for ex in examples[\"translation\"]]\n",
        "    targets = [ex[target_lang] for ex in examples[\"translation\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
        "\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "tokenized_datasets = raw_dataset.map(preprocess_function, batched=True)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-30T13:42:53.724950Z",
          "iopub.execute_input": "2025-07-30T13:42:53.725251Z",
          "iopub.status.idle": "2025-07-30T13:42:57.899690Z",
          "shell.execute_reply.started": "2025-07-30T13:42:53.725201Z",
          "shell.execute_reply": "2025-07-30T13:42:57.898754Z"
        },
        "colab": {
          "referenced_widgets": [
            "e5b1e50fc5fa4c2382004ba951b9ed18",
            "54866169f3564b28aaa7fdf7584aa88e"
          ]
        },
        "id": "-cuUjcLarpqB",
        "outputId": "bba3803d-856a-42e8-974d-c0d8a43b4b36"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/1800 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e5b1e50fc5fa4c2382004ba951b9ed18"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:3950: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/201 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "54866169f3564b28aaa7fdf7584aa88e"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "# Safe version of training arguments for older transformers\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=1,\n",
        "    num_train_epochs=1\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-30T13:42:57.902201Z",
          "iopub.execute_input": "2025-07-30T13:42:57.902537Z",
          "iopub.status.idle": "2025-07-30T13:42:59.022421Z",
          "shell.execute_reply.started": "2025-07-30T13:42:57.902511Z",
          "shell.execute_reply": "2025-07-30T13:42:59.021467Z"
        },
        "id": "Uwo4nq0OrpqB"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=1,\n",
        "    num_train_epochs=1,\n",
        "    predict_with_generate=True,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-30T13:42:59.023253Z",
          "iopub.execute_input": "2025-07-30T13:42:59.023604Z",
          "iopub.status.idle": "2025-07-30T13:42:59.031581Z",
          "shell.execute_reply.started": "2025-07-30T13:42:59.023572Z",
          "shell.execute_reply": "2025-07-30T13:42:59.030315Z"
        },
        "id": "1BI19ySMrpqC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "metric = evaluate.load(\"sacrebleu\")\n",
        "\n",
        "def postprocess_text(preds, labels):\n",
        "    preds = [pred.strip() for pred in preds]\n",
        "    labels = [[label.strip()] for label in labels]\n",
        "    return preds, labels\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "    if isinstance(preds, tuple):\n",
        "        preds = preds[0]\n",
        "\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
        "\n",
        "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    result = {\"bleu\": result[\"score\"]}\n",
        "\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "    return {k: round(v, 4) for k, v in result.items()}\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-30T13:42:59.032656Z",
          "iopub.execute_input": "2025-07-30T13:42:59.032971Z",
          "iopub.status.idle": "2025-07-30T13:42:59.634566Z",
          "shell.execute_reply.started": "2025-07-30T13:42:59.032947Z",
          "shell.execute_reply": "2025-07-30T13:42:59.633283Z"
        },
        "id": "bg8cgal7rpqC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# Evaluate before training\n",
        "initial_eval = trainer.evaluate()\n",
        "print(\" Evaluation Before Training:\")\n",
        "print(initial_eval)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-30T13:42:59.635377Z",
          "iopub.execute_input": "2025-07-30T13:42:59.636335Z",
          "iopub.status.idle": "2025-07-30T14:40:54.528535Z",
          "shell.execute_reply.started": "2025-07-30T13:42:59.636298Z",
          "shell.execute_reply": "2025-07-30T14:40:54.526624Z"
        },
        "id": "YkkWpA_brpqC",
        "outputId": "ce5bf887-c691-4c85-d920-77bce0df12c7"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "/tmp/ipykernel_208/2186300128.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n  trainer = Seq2SeqTrainer(\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='26' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [13/13 1:24:50]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "📊 Evaluation Before Training:\n{'eval_loss': 7.538430213928223, 'eval_model_preparation_time': 0.0036, 'eval_bleu': 0.0157, 'eval_gen_len': 290.8806, 'eval_runtime': 3474.3963, 'eval_samples_per_second': 0.058, 'eval_steps_per_second': 0.004}\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-30T14:40:54.531487Z",
          "iopub.execute_input": "2025-07-30T14:40:54.531865Z",
          "iopub.status.idle": "2025-07-30T14:59:14.488421Z",
          "shell.execute_reply.started": "2025-07-30T14:40:54.531838Z",
          "shell.execute_reply": "2025-07-30T14:59:14.487187Z"
        },
        "id": "Nhuwf28wrpqC",
        "outputId": "7da3580e-06df-4a6b-cb5a-1ea14e70274d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='113' max='113' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [113/113 18:05, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py:3852: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[58100]]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "execution_count": 9,
          "output_type": "execute_result",
          "data": {
            "text/plain": "TrainOutput(global_step=113, training_loss=5.095050102841538, metrics={'train_runtime': 1098.1979, 'train_samples_per_second': 1.639, 'train_steps_per_second': 0.103, 'total_flos': 61017056870400.0, 'train_loss': 5.095050102841538, 'epoch': 1.0})"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "final_eval = trainer.evaluate()\n",
        "print(\"Evaluation After Training:\")\n",
        "print(final_eval)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-30T14:59:14.489854Z",
          "iopub.execute_input": "2025-07-30T14:59:14.490740Z",
          "iopub.status.idle": "2025-07-30T15:12:09.733678Z",
          "shell.execute_reply.started": "2025-07-30T14:59:14.490690Z",
          "shell.execute_reply": "2025-07-30T15:12:09.732582Z"
        },
        "id": "3mTQyit5rpqD",
        "outputId": "ddb0705a-fadb-49d2-c09c-f304c3a6f008"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "📊 Evaluation After Training:\n{'eval_loss': 4.041402816772461, 'eval_model_preparation_time': 0.0036, 'eval_bleu': 0.2878, 'eval_gen_len': 15.6368, 'eval_runtime': 775.2275, 'eval_samples_per_second': 0.259, 'eval_steps_per_second': 0.017, 'epoch': 1.0}\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    }
  ]
}